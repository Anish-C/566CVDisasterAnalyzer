# 5-MINUTE PRESENTATION SCRIPT - RESEARCH FOCUS
## Read this while showing slides

---

## SLIDE 1: RESEARCH QUESTION (0:00-1:00)

"CNNs output a confidence score, but what actually makes them certain?

On hurricane imagery, why does the model insist 'Destroyed' on one roof and hesitate on another that looks similar to us?

Our question: Which visual cues drive certainty vs. uncertainty? Can we open the black box and see what the network is keying on?"

---

## SLIDE 2: HYPOTHESIS (1:00-2:00)

"We think certainty comes from specific visual patterns:

• High-entropy textures that look like structural breakup
• Strong edges between intact and damaged regions
• Consistent damage at the right scale (uniformly destroyed or uniformly intact)

We test whether these actually align with what the model uses."

---

## SLIDE 3: METHOD (2:00-3:00)

"On 2000 hurricane images, we ran three probes:

• Class activation maps: highlight pixels driving each decision; compare high-confidence vs low-confidence cases.

• Feature map inspection: check internal layers—do high-certainty images activate different filters than uncertain ones?

• Ablations: mask regions and measure confidence drop to see which parts make the model sure."

---

## SLIDE 4: FINDINGS (3:00-4:00)

"What we found:

• High confidence shows up on clean, uniform damage or fully intact roofs; partial damage creates uncertainty.

• CAMs show the model fixates on roofline breaks and color/texture contrast—not true structural cues.

• Confidence is highest on visually extreme cases, lowest on borderline ones—opposite of humans.

• Certainty tracks visual distinctiveness, not actual damage severity."

---

## SLIDE 5: IMPLICATIONS (4:00-5:00)

"Why this matters:

• Confidence ≠ correctness. The model can be confidently wrong when visuals look familiar.

• For disaster triage, raw confidence is unsafe. We need better uncertainty (ensembles/Bayesian) and human-in-the-loop checks on low-confidence cases.

• Open questions: Can we disentangle 'visual distinctiveness confidence' from real accuracy? Can disagreeing models flag edge cases automatically?

This is a research step toward interpretable, reliable CV for disasters."

---
---
---

## TECHNICAL NOTES (for Q&A)

**What is ResNet-50?**
- Residual Network with 50 layers (48 conv + 1 pooling + 1 FC)
- Uses skip connections to train very deep networks
- Pretrained on ImageNet (1.4M images, 1000 classes)
- We adapt the final layer for 4-class damage classification (Intact, Minor, Major, Destroyed)
- Standard baseline architecture from 2015 (Microsoft Research)

**Why transfer learning?**
- ResNet-50 already knows low-level features (edges, textures, shapes) from ImageNet
- We fine-tune on hurricane imagery instead of training from scratch
- Faster convergence and better performance with limited disaster data

**Class Activation Maps (CAM)?**
- Heatmap showing which pixels most influence the model's decision
- Reveals what the network "looks at" when making predictions
- Helps us understand if it's using real damage cues or spurious patterns

**Dataset:**
- 2000 labeled hurricane satellite images from Kaggle
- 1000 damaged, 1000 undamaged buildings
- 4-class damage labels: Intact, Minor Damage, Major Damage, Destroyed

**Model Performance:**
- ~85% accuracy on held-out test set
- Confidence scores range 28-39% on uncertain cases
- High confidence (>80%) on extreme damage or fully intact buildings
- Low confidence (30-50%) on partial/borderline damage
